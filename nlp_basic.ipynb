{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vamsi633/NLP-basics/blob/main/nlp_basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**small letter conersion**"
      ],
      "metadata": {
        "id": "k1pZHDvGd06c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "txt=\"Hello World\"\n",
        "print(txt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AobdhxwkbdFj",
        "outputId": "9cb52648-6c04-4293-f392-e5c634db47d4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello World\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x=txt.casefold()\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfzQC0VMbqwY",
        "outputId": "c1371334-42db-4f38-bd4c-1092ff3776cc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello world\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Special Chararacter removal**"
      ],
      "metadata": {
        "id": "BLSmjhNycmvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "t=\"He$ is! a good #boy\"\n",
        "b=re.sub(r\"[^a-zA-Z0-9\\s]\",\"\",t)\n",
        "print(b)\n",
        "\n",
        "\n",
        "t2='HAI# %!!'\n",
        "b1=re.sub(r\"[^a-zA-Z0-9\\s]\",\"\",t2)\n",
        "c1=b1.casefold()\n",
        "print(c1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z71y5FwGc0jG",
        "outputId": "e8fa81eb-ae45-409f-d5b3-3421f65a535c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "He is a good boy\n",
            "hai \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using spacy library for scr and tokenization\n"
      ],
      "metadata": {
        "id": "lNz99EJjmsU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp=spacy.load('en_core_web_sm')\n",
        "inp=\"Hello# how@ are $you!!\"\n",
        "\n",
        "def clean_text(text):\n",
        "  cleaned_text=''.join(char for char in text if char.isalpha() or char.isspace())\n",
        "  doc=nlp(cleaned_text)\n",
        "  return ' '.join(token.text for token in doc)\n",
        "\n",
        "clean_str=clean_text(inp)\n",
        "print(clean_str)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCjsdkv1g1aP",
        "outputId": "fc91285f-b614-43ff-fb35-967c618815e6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n",
            "  warnings.warn(Warnings.W111)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello how are you\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fhs6GZ4qFMx"
      },
      "source": [
        "To execute the code in the above cell, select it with a click and then either press the play button to the left of the code, or use the keyboard shortcut \"Command/Ctrl+Enter\". To edit the code, just click the cell and start editing.\n",
        "\n",
        "Variables that you define in one cell can later be used in other cells:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Using nltk for scr and tokenization**"
      ],
      "metadata": {
        "id": "E9w4nN5unOGq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install nltk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6gyqbohqo1T",
        "outputId": "82b749ee-a9fb-4cf2-9c51-895addc0209c"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fcDHZFXq7GG",
        "outputId": "32fde20f-2a6c-4c37-b1d8-f47cb015d462"
      },
      "execution_count": 7,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NLTK Downloader\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> d\n",
            "\n",
            "Download which package (l=list; x=cancel)?\n",
            "  Identifier> punkt_tab\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "    Downloading package punkt_tab to /root/nltk_data...\n",
            "      Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "---------------------------------------------------------------------------\n",
            "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
            "---------------------------------------------------------------------------\n",
            "Downloader> q\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "input_str='hello how@ are #you!'\n",
        "\n",
        "#tokenization\n",
        "#only if not printing , first try as usaul\n",
        "#if you want to try print tokens and facing problem if not printed then take out punkt in nltk download keep it like this nltk.download() , then select download then type punt_tab.then it will produce tokens\n",
        "tokens=word_tokenize(input_str)\n",
        "\n",
        "#remove special characters\n",
        "clean_tokens=[token for token in tokens if token.isalnum()]\n",
        "\n",
        "clean_str=' '.join(clean_tokens)\n",
        "\n",
        "\n",
        "print(clean_str)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUkKqCfEnMxX",
        "outputId": "81ac9644-0288-49f0-b51a-e521d952b781"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello how are you\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# Download the necessary resources\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Sample text for tokenization\n",
        "text = \"Hello! How are you today? I hope you're having a great day.\"\n",
        "\n",
        "# Tokenize the text into words\n",
        "words = nltk.word_tokenize(text)\n",
        "\n",
        "# Print the tokenized words\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BuhSRebAuT5d",
        "outputId": "485d7a63-7016-4543-bcda-37acfe59baf4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', '!', 'How', 'are', 'you', 'today', '?', 'I', 'hope', 'you', \"'re\", 'having', 'a', 'great', 'day', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Contractions** :- converting shortend form word to norma state.i'm -> i am"
      ],
      "metadata": {
        "id": "RcQhirN11D97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "siOK5aM_uTvM",
        "outputId": "1354703d-e974-4b99-df3c-871a4e9318af"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.1.73-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting textsearch>=0.0.21 (from contractions)\n",
            "  Downloading textsearch-0.0.24-py2.py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting anyascii (from textsearch>=0.0.21->contractions)\n",
            "  Downloading anyascii-0.3.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting pyahocorasick (from textsearch>=0.0.21->contractions)\n",
            "  Downloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
            "Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyahocorasick-2.1.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (118 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.3/118.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.1.0 textsearch-0.0.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import contractions\n",
        "\n",
        "txt=\" hello i'm vamsi and it's raining here\"\n",
        "\n",
        "expanded_text=contractions.fix(txt)\n",
        "\n",
        "print(expanded_text)"
      ],
      "metadata": {
        "id": "Yuh_TnWY1ZB7",
        "outputId": "22b98248-d0e3-4f14-d7dd-3dd49f2db0af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " hello i am vamsi and it is raining here\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization**"
      ],
      "metadata": {
        "id": "85grPGzeLBkJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "nltk.download('punkt')\n",
        "txt='Hello this is vamsikrishna give an internship. it is what i am looking for right now. so please give me a chance'"
      ],
      "metadata": {
        "id": "_5C-R0a-LFy2",
        "outputId": "2b4915e4-f7aa-4394-c450-959dc726da20",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words=word_tokenize(txt)\n",
        "print(words)"
      ],
      "metadata": {
        "id": "cjoQWUMFL7-q",
        "outputId": "2d831003-8087-4555-a4cc-93762c90c520",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', 'this', 'is', 'vamsikrishna', 'give', 'an', 'internship', '.', 'it', 'is', 'what', 'i', 'am', 'looking', 'for', 'right', 'now', '.', 'so', 'please', 'give', 'me', 'a', 'chance']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences=sent_tokenize(txt)\n",
        "print(sentences)"
      ],
      "metadata": {
        "id": "BeXEdZlAMmZd",
        "outputId": "79916f70-25eb-49da-8e7b-996a7e937294",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello this is vamsikrishna give an internship.', 'it is what i am looking for right now.', 'so please give me a chance']\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}